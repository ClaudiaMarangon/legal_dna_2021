{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02. Tokens and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencing Legal DNA: NLP for Law and Political Economy<br>\n",
    "Elliott Ash, ETH Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data from lesson 1.\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Basic Pre-Processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Prof. Zurich hailed from Zurich. She got 3 M.A.'s from ETH.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK has a fast implementation that makes errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text) # split document into sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spacy works better.**\n",
    "\n",
    "**Install spacy and the English model if you have not already.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing capitalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalization\n",
    "text_lower = text.lower() # go to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Punctuation\n",
    "#####\n",
    "\n",
    "# recipe for fast punctuation removal\n",
    "from string import punctuation\n",
    "print (\"punctuation:\", punctuation)\n",
    "punc_remover = str.maketrans('','',punctuation) \n",
    "text_nopunc = text_lower.translate(punc_remover)\n",
    "print(text_nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens\n",
    "tokens = text_nopunc.split() # splits a string on white space\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers\n",
    "# remove numbers (keep if not a digit)\n",
    "no_numbers = [t for t in tokens if not t.isdigit()]\n",
    "# keep if not a digit, else replace with \"#\"\n",
    "norm_numbers = [t if not t.isdigit() else '#' \n",
    "                for t in tokens ]\n",
    "print(no_numbers )\n",
    "print(norm_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english') \n",
    "print (\"stop words:\", stoplist)\n",
    "# keep if not a stopword\n",
    "nostop = [t for t in norm_numbers if t not in stoplist]\n",
    "print(nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn stopwords\n",
    "# depending on sklearn version, for sklearn==0.24.1, stop_words are here\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "sorted(list(stop_words))[:20]\n",
    "\n",
    "# in older versions, one may hvae to import it like this\n",
    "# from sklearn.feature_extraction import stop_words\n",
    "# sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy stopwords\n",
    "sorted(list(nlp.Defaults.stop_words))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english') # snowball stemmer, english\n",
    "# remake list of tokens, replace with stemmed versions\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens]\n",
    "print(tokens_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('german') # snowball stemmer, german\n",
    "print(stemmer.stem(\"Autobahnen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize('corporation'), wnl.lemmatize('corporations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap it into a recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply it to the Supreme Court Cases Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_cleaned'] = df['opinion_text'].apply(normalize_text)\n",
    "df['tokens_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shortcut: `gensim.simple_preprocess`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "print(simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(simple_preprocess(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `simple_preprocess` on the Supreme Court corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_simple'] = df['opinion_text'].apply(simple_preprocess)\n",
    "df['tokens_simple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[flair NLP](https://github.com/flairNLP/flair)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair\n",
    "# simple tokenization\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence('The grass is green.')\n",
    "print(sentence)\n",
    "\n",
    "for token in sentence:\n",
    "        print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Science cannot solve the ultimate mystery of nature. And that is because, in the last analysis, we ourselves are a part of the mystery that we are trying to solve.'\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import perceptron \n",
    "from nltk import word_tokenize\n",
    "tagger = perceptron.PerceptronTagger()\n",
    "tokens = word_tokenize(text)\n",
    "tagged_sentence = tagger.tag(tokens)\n",
    "tagged_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot nouns and adjectives over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_nouns_adj(snippet):\n",
    "    tags = [x[1] for x in tagger.tag(word_tokenize(snippet))]\n",
    "    num_nouns = len([t for t in tags if t[0] == 'N'])\n",
    "    num_adj = len([t for t in tags if t[0] == 'J'])\n",
    "    return num_nouns, num_adj\n",
    "\n",
    "df['nouns'], df['adj'] = zip(*df['opinion_text'].map(get_nouns_adj))\n",
    "df.groupby('year')[['nouns','adj']].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of nouns, adjectives, and verbs from WordNet\n",
    "# nltk.download('wordnet')\n",
    "from nltk import wordnet as wn\n",
    "\n",
    "nouns = set([x.lemma_names()[0].lower() \n",
    "             for x in wn.wordnet.all_synsets('n')])\n",
    "\n",
    "adjectives = set([x.lemma_names()[0].lower() \n",
    "             for x in wn.wordnet.all_synsets('a')])\n",
    "\n",
    "verbs = set([x.lemma_names()[0].lower() \n",
    "             for x in wn.wordnet.all_synsets('v')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Prep with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get spacy documents for each speech and add to dataframe. This is quicker than iterating over the dataframe with `iterrows()`, but slower than a parallelized solution. It will take a few minutes for a whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(10)\n",
    "dfs['doc'] = dfs['opinion_text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The spacy model already gives you sentences and tokens.\n",
    "# For example:\n",
    "sent1 = list(dfs['doc'].iloc[0].sents)[1]\n",
    "sent1 # sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "list(sent1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas\n",
    "[x.lemma_ for x in sent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags\n",
    "[x.tag_ for x in sent1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing a Corpus with spaCy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['num_words'] = dfs['doc'].apply(lambda x: len(list(x)))\n",
    "dfs['num_words'] = dfs['doc'].apply(lambda x: len(list(x.sents)))\n",
    "\n",
    "\n",
    "print(len(tokens),'words in corpus.')\n",
    "words_per_sent = len(tokens) / len(sentences)\n",
    "print(words_per_sent,'words per sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre-processing with spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x, nlp):\n",
    "    # lemmatize and lowercase without stopwords, punctuation and numbers\n",
    "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
    "tokenize(text, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# get n-gram counts for 10 documents\n",
    "grams = []\n",
    "for i, row in df.iterrows():\n",
    "    tokens = row['opinion_text'].lower().split() # get tokens\n",
    "    for n in range(2,4):\n",
    "        grams += list(ngrams(tokens,n)) # get bigrams, trigrams, and quadgrams\n",
    "    if i > 50:\n",
    "        break\n",
    "Counter(grams).most_common()[:8]  # most frequent n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter is a quick pure-python solution.\n",
    "from collections import Counter\n",
    "freqs = Counter(tokens)\n",
    "freqs.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use scikit-learn's vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,3))\n",
    "X = vec.fit_transform(df['opinion_text'])\n",
    "\n",
    "# save the vectors\n",
    "pd.to_pickle(X,'X.pkl')\n",
    "\n",
    "# save the vectorizer \n",
    "# (so you can transform other documents, \n",
    "# also for the vocab)\n",
    "pd.to_pickle(vec, 'vec-3grams-1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf vectorizer up-weights rare/distinctive words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df['opinion_text'])\n",
    "pd.to_pickle(X_tfidf,'X_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make word cloud of common words by topic id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_id'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tfidf.get_feature_names()\n",
    "vocab[:10], vocab[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for topic_id in [1,2,8,9]: \n",
    "    slicer = df['topic_id'] == topic_id\n",
    "    f = X_tfidf[slicer.values]\n",
    "    total_freqs = list(np.array(f.sum(axis=0))[0])\n",
    "    fdict = dict(zip(vocab,total_freqs))\n",
    "    # generate word cloud of words with highest counts\n",
    "    wordcloud = WordCloud().generate_from_frequencies(fdict) \n",
    "    print(topic_id)\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear') \n",
    "    plt.axis(\"off\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make word cloud of common words by naive bayes topic ranking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "# X is the data transformed by CountVectorizer above, y are the topic_ids\n",
    "print (X.shape, df['topic_id'].astype(int))\n",
    "nb.fit(X.todense(), df['topic_id'].astype(int))\n",
    "df[\"topic_id\"].describe()\n",
    "print (nb.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in [1,2,8,9]: \n",
    "    # get feature coefficients for topic_id from naive bayes classifier\n",
    "    f = nb.coef_[topic_id]\n",
    "    fdict = dict(zip(vocab,f))\n",
    "    # generate word cloud of words with highest feature coefficients\n",
    "    wordcloud = WordCloud().generate_from_frequencies(fdict) \n",
    "    print(topic_id)\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud, interpolation='bilinear') \n",
    "    plt.axis(\"off\") \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer tokenizers and stemmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "X_hash = hv.fit_transform(df['opinion_text'])\n",
    "X_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict vectorizer\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "#TODO make this work\n",
    "#dv = DictVectorizer()\n",
    "#X_dv = dv.fit_transform(df['opinion_text'])\n",
    "#X_dv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**debugging hashing vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5\n",
    "from eli5.sklearn import InvertableHashingVectorizer\n",
    "\n",
    "ivec = InvertableHashingVectorizer(hv)\n",
    "inverted_hv = ivec.fit(df['opinion_text'])\n",
    "print ([i for i in inverted_hv.get_feature_names()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hashing with keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "text = \"Prof. Zurich hailed from Zurich. She got 3 M.A.'s from ETH.\"\n",
    "n = 5 # num features\n",
    "tf.keras.preprocessing.text.hashing_trick(\n",
    "    text, n, hash_function=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' '\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collocations: Point-Wise Mutual Information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "from functools import reduce\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def get_gmean(phrase, termfreqs):\n",
    "    words = phrase.split('_')\n",
    "    n = len(words)\n",
    "    p = [termfreqs[w]**(1/n) for w in words]\n",
    "    numerator = termfreqs[phrase]   \n",
    "    denominator = reduce(mul, p)\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    gmean = numerator / denominator\n",
    "    return gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS-filtered N-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Normalized Penn tags\n",
    "tagdict = { 'NN':'N',\n",
    "            'NNS':'N',\n",
    "                                    \n",
    "            'JJ':'A',\n",
    "            'JJR':'A',\n",
    "            'JJS':'A',\n",
    "            'VBG':'A', # gerunds/participles treated like adjectives\n",
    "\n",
    "            'RB':'A', # adverbs treated as adjectives\n",
    "            'RBR':'A',\n",
    "            'RBS':'A',\n",
    "            'PDT':'A', # predeterminer            \n",
    "\n",
    "            'VB':'V',\n",
    "            'VBD':'V',\n",
    "            'VBN':'V',\n",
    "            'VBP':'V',\n",
    "            'VBZ':'V',\n",
    "            'MD': 'V', # modals treated as verbs\n",
    "            'RP': 'V', # particles treated as verbs\n",
    "            \n",
    "            'DT':'D',\n",
    "                        \n",
    "            'IN':'P',\n",
    "            'TO':'P',\n",
    "\n",
    "            'CC': 'C'}\n",
    "\n",
    "tagpatterns = {'A','N','V','P','C','D',\n",
    "           'AN','NN', 'VN', 'VV', \n",
    "            #'NV',\n",
    "            'VP',                                    \n",
    "            'NNN','AAN','ANN','NAN','NPN',\n",
    "            'VAN','VNN', 'AVN', 'VVN',\n",
    "            'VPN', 'VDN', \n",
    "            #'ANV','NVV','VVV', 'NNV',\n",
    "            'VVP','VAV','VVN',\n",
    "            'NCN','VCV', 'ACA',  \n",
    "            'PAN',\n",
    "            'NCVN','ANNN','NNNN','NPNN', 'AANN' 'ANNN','ANPN','NNPN','NPAN', \n",
    "            'ACAN', 'NCNN', 'NNCN', 'ANCN', 'NCAN',\n",
    "            'PDAN', 'PNPN',\n",
    "            'VDNN', 'VDAN','VVDN'}\n",
    "\n",
    "def count_pos_grams(sentence, max_phrase_length=4):\n",
    "    sent_freq = Counter()\n",
    "    tagwords = []\n",
    "    for (word,tag) in tagger.tag(sentence):\n",
    "        if tag in tagdict:\n",
    "            normtag = tagdict[tag]\n",
    "            stemmed = word.lower()#stemmer.stem(word)\n",
    "            tagwords.append((stemmed,normtag))\n",
    "        else:\n",
    "            tagwords.append(None)\n",
    "    for n in range(1,max_phrase_length+1):            \n",
    "        rawgrams = ngrams(tagwords,n)\n",
    "        for rawgram in rawgrams:\n",
    "            # skip grams that have rare words\n",
    "            if None in rawgram:\n",
    "                continue\n",
    "            gramtags = ''.join([x[1][0] for x in rawgram])\n",
    "            if gramtags in tagpatterns:\n",
    "                 # if tag sequence is allowed, add to counter\n",
    "                gram = '_'.join([x[0] for x in rawgram])\n",
    "                sent_freq[gram] += 1\n",
    "    return sent_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_phrase_length = 4\n",
    "termfreqs = Counter()\n",
    "\n",
    "for i, doc in dfs.iterrows():    \n",
    "    termfreqs.update(count_pos_grams(nltk.word_tokenize(doc['opinion_text'])))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unigrams\n",
    "grams = [x for x in termfreqs.most_common() if '_' in x[0]]\n",
    "# make dataframe of geometric mean associations for each gram\n",
    "gmeans = pd.DataFrame([(gram[0], get_gmean(gram[0],termfreqs)) for gram in grams],\n",
    "              columns=['ngram','gmean'])\n",
    "gmeans.sort_values('gmean',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer can be run directly on tokens so you can run it on the phrased documents, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vec = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                            max_df=.9,  # no tokens appearing in more than 90% of all docs\n",
    "                            max_features=10000,                                                \n",
    "                            preprocessor =lambda x: x, # for tokens\n",
    "                            tokenizer = lambda x: x, # for tokens\n",
    "                            stop_words='english',\n",
    "                            ngram_range=(1,1))\n",
    "#print (df[\"tokens_cleaned\"][0])\n",
    "#print (df[\"opinion_text\"][0])\n",
    "#print (df.columns)\n",
    "X_pos = vec.fit_transform(df['opinion_text'])\n",
    "X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy noun chunks\n",
    "i = 0\n",
    "for chunk in nlp(doc['opinion_text']).noun_chunks:\n",
    "    print ('{} - {}'.format(chunk, chunk.label_))\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy prepositional phrase chunking\n",
    "# find all PPs (e.g. \"in the court\")\n",
    "def get_pps(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        # if we have a prepositional object and the dependency head of the current token is a preposition, we have a prepositional phrase\n",
    "        if token.dep_ == \"pobj\" and token.head.dep_ == \"prep\":\n",
    "            # we just iterate through the subtree then and collect the dependency head, the token itself and all tokens in the subtree\n",
    "            pp = token.head.text + \" \" + ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            print (pp)\n",
    "            \n",
    "get_pps(df[\"opinion_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy named entities\n",
    "i = 0\n",
    "for entity in nlp(doc['opinion_text']).ents:\n",
    "    print ('{} - {}'.format(entity, entity.label_))\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flair NER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple tokenization\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence('George Washington went to Washington .')\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "tagger.predict(sentence)\n",
    "print(sentence.to_tagged_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Univariate feature selection using chi2\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression, f_classif, mutual_info_classif\n",
    "select = SelectKBest(chi2, k=10)\n",
    "Y = df['cite_count']\n",
    "X_new = select.fit_transform(X, Y)\n",
    "# top 10 features by chi-squared:\n",
    "[vocab[i] for i in np.argsort(select.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% top 10 features by  ANOVA F-value:\n",
    "select = SelectKBest(f_classif, k=10)\n",
    "select.fit(X, Y)\n",
    "[vocab[i] for i in np.argsort(select.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% top 10 features by linear regression\n",
    "select = SelectKBest(f_regression, k=10)\n",
    "select.fit(X, Y)\n",
    "[vocab[i] for i in np.argsort(select.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% top 10 features by mutual information (classification)\n",
    "select = SelectKBest(mutual_info_classif, k=10)\n",
    "select.fit(X[:1000], Y[:1000])\n",
    "[vocab[i] for i in np.argsort(select.scores_)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# OLS Regression\n",
    "###\n",
    "\n",
    "# list of words from our vectorizer\n",
    "#vocab = [w.replace(' ', '_') for w in vocab]\n",
    "         \n",
    "# convert frequency counts to dataframe\n",
    "#df4 = pd.DataFrame(X.todense(),\n",
    "#                   columns=vocab)\n",
    "\n",
    "# import statsmodels package for R-like regression formulas\n",
    "#import statsmodels.formula.api as smf\n",
    "\n",
    "# add metadata\n",
    "#df4['Y'] = df['log_cite_count'] # cites to this opinion\n",
    "#df4['judgefe'] = df['authorship']   # judge fixed effect\n",
    "#df4['yearfe'] = pd.to_datetime(df['date_standard']).dt.year        # year fixed effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists for t-statistics and coefficients\n",
    "#tstats, betas = [], []\n",
    "\n",
    "#for xvar in vocab: # loop through the words in vocab\n",
    "#    if any([c.isdigit() for c in xvar]) or 'hellip' in xvar:\n",
    "#        tstats.append(0)\n",
    "#        betas.append(0)\n",
    "#        continue\n",
    "#    model = smf.ols('Y ~ %s' % xvar,data=df4)                \n",
    "#    result = model.fit() \n",
    "#    tstats.append(result.tvalues[1])\n",
    "#    betas.append(result.params[1])\n",
    "            \n",
    "# zip up words and t-statistics\n",
    "#stats = list(zip(vocab,tstats))\n",
    "#stats.sort(key = lambda x: x[1], reverse=True) # sort by second item (tstats)\n",
    "#stats[:10] + stats[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentencepiece Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "# training spm requires a text file as input, so generate a small one\n",
    "with open(\"sample_text.txt\", \"w\") as outfile:\n",
    "        for text in df[\"opinion_text\"][:100]:\n",
    "            outfile.write(text + \"\\n\")\n",
    "            \n",
    "spm.SentencePieceTrainer.train(input=\"sample_text.txt\", model_prefix='m', vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "sp.encode_as_pieces(df[\"opinion_text\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp.encode_as_ids(df[\"opinion_text\"][0][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Huggingface tokenizers](https://huggingface.co/transformers/main_classes/tokenizer.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface is a very nice library built around transformers and allows us to do pretty much anything with it. All different models consist of model parameters, model code (e.g., different attention mechanisms) and a unique tokenizer. We will re-visit huggingface later in this class, but give an intro for wordpiece tokenization using huggingface in the following section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use distilbert tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# tokenize text\n",
    "text = \"Prof. Zurich hailed from Zurich. She got 3 M.A.'s from ETH.\"\n",
    "tokenizer.tokenize(text) #word pieces start with ##..., e.g. ETH is split into et ##h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizers are callable and will transform raw text input to the model input (e.g. input_ids, attention_mask, token_segment_ids)\n",
    "model_inputs = tokenizer(text)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can then decode input_ids back to text\n",
    "tokenizer.decode(model_inputs[\"input_ids\"]) \n",
    "# note how we added two special tokens to the input, the [CLS] and [SEP] tokens\n",
    "# this will be important later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
